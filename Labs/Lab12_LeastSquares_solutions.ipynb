{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOggqQP5GIVFohGSvFjjA3g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cu-applied-math/appm-4600-numerics/blob/lab_solutions/Labs/Lab12_LeastSquares_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 12: Least Squares (SOLUTIONS)\n",
        "\n",
        "This lab focuses on the problem\n",
        "$$\\min_{\\vec{x}\\in\\mathbb{R}^m}\\;\\|A\\vec{x}-\\vec{b}\\|_2$$\n",
        "where $A\\in\\mathbb{R}^{m\\times n}$, and $m\\ge n$ and $\\text{rank}(A)=n$.\n",
        "\n",
        "We'll make problems where we can check our answer, first creating a \"true\" $\\vec{x}_{\\text{true}}$, then set $\\vec{b}=A \\vec{x}_{\\text{true}}$ for a given $A$ matrix, and your job is to solve for\n",
        "$\\vec{x}_{\\text{estimate}}=\\text{argmin}_{\\vec{x}}\\|A\\vec{x}-\\vec{b}\\|_2$\n",
        "\n",
        "#### Learning objectives\n",
        "- Learn how to solve a least squares problem \"from scratch\" (well, partially from scratch)\n",
        "- Understand pros and cons of different methods\n",
        "  - Which ones require pivoting?\n",
        "  - Which ones are faster?\n",
        "  - Which ones are faster on multiple cores or a GPU?  (i.e., more parallelizable)\n",
        "  - Which ones are more accurate?\n",
        "  - Does this depend on conditioning of $A$? On its shape? (i.e., almost square vs very tall and skinny)\n",
        "\n",
        "#### Tasks\n",
        "1. Write at least **four** distinct implementations to solve the least-squares problem (the solutions have about implementations). The implementations can be similar to each other, but each variant should be non-trivial.  You **are** allowed to use linear algebra library functions, such as:\n",
        "    - `scipy.linalg.inverse`\n",
        "    - `scipy.linalg.solve` and all its variants (i.e., the different options for `assume_a`)\n",
        "    - `scipy.linalg.qr` and all its variants (i.e., pivoting or not)\n",
        "    - `scipy.linalg.solve_triangular` for forward/back substitution\n",
        "    - `scipy.linalg.qr_multiply`\n",
        "    - `scipy.linalg.svd`\n",
        "    - `scipy.linalg.lstsq` and all its variants (i.e., the different options for `lapack_driver`). Since this routine does everything for you, you may only count it as **one** of your implementations (that is, if you do different options for `lapack_driver`, those don't count as separate implementations).\n",
        "2. Use your implementations\n",
        "    - Try \"problem 0\", a small easy problem that is good for checking whether your code works\n",
        "    - Then try \"problem 1\" and \"problem 2\" which are larger and less well-conditioned. How well do the implementations work?  \n",
        "        - **Time** your code\n",
        "        - Also **record the error** in the $\\ell_\\infty$ norm, i.e., $\\|\\vec{x_{true}} - \\vec{x_{est}}\\|_\\infty$\n",
        "3. If you have time, try an implementation on the GPU. *We highly recommend that this part of the lab is done on google colab*\n",
        "    - Use pytorch (already installed on colab)\n",
        "    - Select a GPU runtime\n",
        "    - See the scaffolded code below\n",
        "\n",
        "#### Deliverables\n",
        "1. Code for at least 4 implementations, and...\n",
        "2. at least one sentence (or more) about some pros and cons of different methods.\n",
        "\n",
        "Put both these deliverables into the same PDF and upload to Canvas\n",
        "\n",
        "*APPM 4600. Copyright 2025 Department of Applied Mathematics, University of Colorado Boulder. Released under a BSD 3-clause license*"
      ],
      "metadata": {
        "id": "sgFygRTS5Vfj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RJcWHZPZ5LhS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.linalg as sla"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: write at least **four** implementations to solve the least-squares problem\n",
        "\n"
      ],
      "metadata": {
        "id": "geE2BIDblDBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 0: use this for debugging\n",
        "problem_label = 'Problem 0'\n",
        "rng     = np.random.default_rng(12345)\n",
        "n       = 20\n",
        "m       = 40\n",
        "A       = rng.standard_normal( (m,n), dtype=np.double )\n",
        "xTrue   = rng.standard_normal(n)\n",
        "b       = A@xTrue\n",
        "print(f'Matrix is size {m} x {n}')\n",
        "print(f'Condition number of A is {np.linalg.cond(A):.2e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9XE-H1ukmqr",
        "outputId": "c0a4c228-5ff0-4380-cf89-6dc319d412eb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix is size 40 x 20\n",
            "Condition number of A is 3.78e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solutions\n",
        "\n",
        "x = ... TODO ...\n",
        "\n",
        "err   = np.linalg.norm(x-xTrue,np.inf)\n",
        "print(f'Error is {err:.2e}')"
      ],
      "metadata": {
        "id": "578eOrA0lRpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLUTIONS\n",
        "\n",
        "all_methods = []\n",
        "\n",
        "def normal_equations_inverse(A,b):\n",
        "    return np.linalg.inv( A.T @ A ) @ (A.T@b )\n",
        "normal_equations_inverse.label = 'Solve normal equations using the inverse'\n",
        "all_methods.append( normal_equations_inverse )\n",
        "\n",
        "def normal_equations_solve1(A,b):\n",
        "    return sla.solve( A.T@A, A.T@b)\n",
        "normal_equations_solve1.label='Solve normal equations using LU'\n",
        "all_methods.append( normal_equations_solve1 )\n",
        "\n",
        "def normal_equations_solve2(A,b):\n",
        "    return sla.solve( A.T@A, A.T@b, assume_a='symmetric')\n",
        "normal_equations_solve2.label='Solve normal equations using LDL'\n",
        "all_methods.append( normal_equations_solve2 )\n",
        "\n",
        "def normal_equations_solve3(A,b):\n",
        "    # No pivoting needed! Should be faster\n",
        "    return sla.solve( A.T@A, A.T@b, assume_a='positive definite')\n",
        "normal_equations_solve3.label='Solve normal equations using Cholesky'\n",
        "all_methods.append( normal_equations_solve3 )\n",
        "\n",
        "def QR_nopivoting(A,b):\n",
        "    # Q,R = numpy.linalg.qr( A, mode=\"reduced\")\n",
        "    Q,R = sla.qr( A, mode=\"economic\") # scipy version slightly different\n",
        "    Qtb = Q.T@b\n",
        "    return sla.solve_triangular(R, Qtb )\n",
        "QR_nopivoting.label='Solve via thin QR, no pivoting'\n",
        "all_methods.append( QR_nopivoting )\n",
        "\n",
        "def QR_pivoting(A,b):\n",
        "    Q,R,pivots = sla.qr( A, mode=\"economic\",pivoting=True)\n",
        "    # so A[:, P] = Q @ R\n",
        "    Qtb = Q.T@b\n",
        "    x   = np.empty(A.shape[1])\n",
        "    x[pivots]   = sla.solve_triangular(R, Qtb )\n",
        "    return x\n",
        "QR_pivoting.label='Solve via thin QR, with pivoting'\n",
        "all_methods.append( QR_pivoting )\n",
        "\n",
        "def QR_nopivoting_fast(A,b):\n",
        "    Qtb, R = sla.qr_multiply( A, b, mode=\"right\", pivoting=False )\n",
        "    return sla.solve_triangular(R, Qtb )\n",
        "QR_nopivoting_fast.label='Solve via thin QR, no pivoting, keeping Q implicit'\n",
        "all_methods.append( QR_nopivoting_fast )\n",
        "\n",
        "def QR_pivoting_fast(A,b):\n",
        "    Qtb, R, pivots = sla.qr_multiply( A, b, mode=\"right\", pivoting=True )\n",
        "    x   = np.empty(A.shape[1])\n",
        "    x[pivots]   = sla.solve_triangular(R, Qtb )\n",
        "    return x\n",
        "QR_pivoting_fast.label='Solve via thin QR, with pivoting, keeping Q implicit'\n",
        "all_methods.append( QR_pivoting_fast )\n",
        "\n",
        "def SVD(A,b):\n",
        "  U,S,Vh = sla.svd(A,full_matrices=False)\n",
        "  return Vh.T @ (np.diag(1/S) @ (U.T @ b) )\n",
        "SVD.label='Solve via SVD'\n",
        "all_methods.append( SVD )\n",
        "\n",
        "def builtin_leastsquares(A,b):\n",
        "    \"\"\" least squares using SVD, divide and conquer \"\"\"\n",
        "    # np.linalg.lstsq(A,b, rcond=None)[0] # numpy slightly different than scipy\n",
        "    # Default driver is gelsd\n",
        "    # https://www.netlib.org/lapack/explore-html/d9/d67/group__gelsd.html\n",
        "    return sla.lstsq(A,b)[0]\n",
        "builtin_leastsquares.label='Builtin least-squares solver, gelsd driver'\n",
        "all_methods.append( builtin_leastsquares )\n",
        "\n",
        "def builtin_leastsquares2(A,b):\n",
        "    \"\"\"least squares using complete orthogonal factor \"\"\"\n",
        "    # https://www.netlib.org/lapack/explore-html/dc/d8b/group__gelsy.html\n",
        "    return sla.lstsq(A,b,lapack_driver='gelsy')[0]\n",
        "builtin_leastsquares2.label='Builtin least-squares solver, gelsy driver'\n",
        "all_methods.append( builtin_leastsquares2 )\n",
        "\n",
        "def builtin_leastsquares3(A,b):\n",
        "    \"\"\" least squares using SVD, QR iteration \"\"\"\n",
        "    # https://www.netlib.org/lapack/explore-html/da/d55/group__gelss.html\n",
        "    return sla.lstsq(A,b,lapack_driver='gelss')[0]\n",
        "builtin_leastsquares3.label='Builtin least-squares solver, gelss driver'\n",
        "all_methods.append( builtin_leastsquares3 )"
      ],
      "metadata": {
        "id": "RMeB7rDf5TXt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Try solving some larger problems\n",
        "- Record the **error** in the infinity norm, and...\n",
        "- Record the **time** it takes\n",
        "\n",
        "To measure time, in jupyter/colab/iPython, you can use a line like\n",
        "\n",
        "`%time x = myFunction(...)`\n",
        "\n",
        "and it will time how long it takes `myFunction(...)` to run\n",
        "\n",
        "\n",
        "What **observations** can you make?\n",
        "\n",
        "Note that if you use `%time`, it will return both the:\n",
        "1. CPU time, which is the sum of how long all cores on the computer spend. So if you have 4 cores, and each core takes 3 seconds (and is fully utilized), the CPU time is 12 seconds.\n",
        "2. \"Wall\" time, which refers to \"wall clock\" time. This is the actual time elapsed from when you start to when you stop.\n",
        "\n",
        "If the CPU time is 12 seconds and the wall time is 3 seconds, this indicates that the code ran in parallel."
      ],
      "metadata": {
        "id": "U-i8EemsrqLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1\n",
        "problem_label = 'Problem 1'\n",
        "rng     = np.random.default_rng(12345)\n",
        "# n       = int(5e3)\n",
        "n       = int(1e3)\n",
        "m       = n+1\n",
        "A       = rng.standard_normal( (m,n), dtype=np.single )\n",
        "xTrue   = rng.standard_normal(n)\n",
        "b       = A@xTrue\n",
        "print(f'Matrix is size {m} x {n}')\n",
        "print(f'Condition number of A is {np.linalg.cond(A):.2e}') # takes a bit of time to run! It's about 4.6e3 if n=5e3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY4d9uZA5ZaE",
        "outputId": "e0148342-d7e3-4666-f933-a133479273f4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix is size 1001 x 1000\n",
            "Condition number of A is 1.24e+03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(problem_label)\n",
        "for method in all_methods:\n",
        "    print('=== ' + method.label + ' ===')\n",
        "    %time x = method(A,b)\n",
        "    err   = np.linalg.norm(x-xTrue,np.inf)\n",
        "    print(f'and l_inf error is {err:.2e}')\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hpl_o-o5eb4",
        "outputId": "0bdd6f47-c1c9-4253-d011-7f47ef2168f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 1\n",
            "=== Solve normal equations using the inverse ===\n",
            "CPU times: user 297 ms, sys: 0 ns, total: 297 ms\n",
            "Wall time: 227 ms\n",
            "and l_inf error is 5.21e-03\n",
            "\n",
            "=== Solve normal equations using LU ===\n",
            "CPU times: user 332 ms, sys: 0 ns, total: 332 ms\n",
            "Wall time: 168 ms\n",
            "and l_inf error is 4.22e-03\n",
            "\n",
            "=== Solve normal equations using LDL ===\n",
            "CPU times: user 377 ms, sys: 1.98 ms, total: 379 ms\n",
            "Wall time: 196 ms\n",
            "and l_inf error is 4.22e-03\n",
            "\n",
            "=== Solve normal equations using Cholesky ===\n",
            "CPU times: user 166 ms, sys: 10 µs, total: 166 ms\n",
            "Wall time: 99.7 ms\n",
            "and l_inf error is 4.22e-03\n",
            "\n",
            "=== Solve via thin QR, no pivoting ===\n",
            "CPU times: user 249 ms, sys: 1 ms, total: 250 ms\n",
            "Wall time: 133 ms\n",
            "and l_inf error is 5.79e-05\n",
            "\n",
            "=== Solve via thin QR, with pivoting ===\n",
            "CPU times: user 416 ms, sys: 28 µs, total: 416 ms\n",
            "Wall time: 210 ms\n",
            "and l_inf error is 3.90e-05\n",
            "\n",
            "=== Solve via thin QR, no pivoting, keeping Q implicit ===\n",
            "CPU times: user 228 ms, sys: 35 µs, total: 228 ms\n",
            "Wall time: 114 ms\n",
            "and l_inf error is 7.07e-05\n",
            "\n",
            "=== Solve via thin QR, with pivoting, keeping Q implicit ===\n",
            "CPU times: user 211 ms, sys: 1 µs, total: 211 ms\n",
            "Wall time: 108 ms\n",
            "and l_inf error is 6.16e-05\n",
            "\n",
            "=== Solve via SVD ===\n",
            "CPU times: user 853 ms, sys: 1.06 ms, total: 854 ms\n",
            "Wall time: 436 ms\n",
            "and l_inf error is 6.67e-05\n",
            "\n",
            "=== Builtin least-squares solver, gelsd driver ===\n",
            "CPU times: user 989 ms, sys: 3.02 ms, total: 992 ms\n",
            "Wall time: 517 ms\n",
            "and l_inf error is 2.03e-13\n",
            "\n",
            "=== Builtin least-squares solver, gelsy driver ===\n",
            "CPU times: user 355 ms, sys: 1.01 ms, total: 356 ms\n",
            "Wall time: 181 ms\n",
            "and l_inf error is 8.37e-14\n",
            "\n",
            "=== Builtin least-squares solver, gelss driver ===\n",
            "CPU times: user 7.47 s, sys: 32.5 ms, total: 7.51 s\n",
            "Wall time: 6.94 s\n",
            "and l_inf error is 2.01e-13\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2\n",
        "problem_label = 'Problem 2'\n",
        "rng = np.random.default_rng(12345)\n",
        "\n",
        "# n   = int(2e3)\n",
        "n   = int(1e3)\n",
        "n2  = int(n/2)\n",
        "# m   = int(1e4)\n",
        "m   = int(5e3)\n",
        "A       = rng.standard_normal( (m,n2) )\n",
        "A       = np.hstack(  (A,A+1e-3*rng.standard_normal( (m,n2) )), dtype=np.single )\n",
        "xTrue   = rng.standard_normal(n)\n",
        "b       = A@xTrue\n",
        "print(f'Matrix is size {m} x {n}')\n",
        "print(f'Condition number of A is {np.linalg.cond(A):.2e}') # takes a bit of time to run! It's about 4.1e4 when m=1e4, 4.1e3 when m=5000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKhJRpYS5i12",
        "outputId": "ca41250e-4354-4f6c-d237-83854fb60d99"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix is size 5000 x 1000\n",
            "Condition number of A is 4.14e+03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(problem_label)\n",
        "for method in all_methods:\n",
        "    print('=== ' + method.label + ' ===')\n",
        "    %time x = method(A,b)\n",
        "    err   = np.linalg.norm(x-xTrue,np.inf)\n",
        "    print(f'and l_inf error is {err:.2e}')\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi2hMAUt5kIm",
        "outputId": "7a757906-3472-48e6-c5a1-55fb55d0aaf7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 2\n",
            "=== Solve normal equations using the inverse ===\n",
            "CPU times: user 535 ms, sys: 9.91 ms, total: 545 ms\n",
            "Wall time: 563 ms\n",
            "and l_inf error is 4.65e+01\n",
            "\n",
            "=== Solve normal equations using LU ===\n",
            "CPU times: user 557 ms, sys: 9.65 ms, total: 566 ms\n",
            "Wall time: 438 ms\n",
            "and l_inf error is 4.42e+01\n",
            "\n",
            "=== Solve normal equations using LDL ===\n",
            "CPU times: user 532 ms, sys: 11.7 ms, total: 544 ms\n",
            "Wall time: 438 ms\n",
            "and l_inf error is 4.42e+01\n",
            "\n",
            "=== Solve normal equations using Cholesky ===\n",
            "CPU times: user 452 ms, sys: 7.64 ms, total: 460 ms\n",
            "Wall time: 346 ms\n",
            "and l_inf error is 4.42e+01\n",
            "\n",
            "=== Solve via thin QR, no pivoting ===\n",
            "CPU times: user 1.2 s, sys: 7.89 ms, total: 1.21 s\n",
            "Wall time: 653 ms\n",
            "and l_inf error is 6.08e-04\n",
            "\n",
            "=== Solve via thin QR, with pivoting ===\n",
            "CPU times: user 2.86 s, sys: 11 ms, total: 2.87 s\n",
            "Wall time: 1.5 s\n",
            "and l_inf error is 9.43e-04\n",
            "\n",
            "=== Solve via thin QR, no pivoting, keeping Q implicit ===\n",
            "CPU times: user 776 ms, sys: 62 µs, total: 776 ms\n",
            "Wall time: 393 ms\n",
            "and l_inf error is 4.01e-04\n",
            "\n",
            "=== Solve via thin QR, with pivoting, keeping Q implicit ===\n",
            "CPU times: user 2.28 s, sys: 4.04 ms, total: 2.29 s\n",
            "Wall time: 1.18 s\n",
            "and l_inf error is 6.80e-04\n",
            "\n",
            "=== Solve via SVD ===\n",
            "CPU times: user 2.25 s, sys: 20.9 ms, total: 2.27 s\n",
            "Wall time: 1.17 s\n",
            "and l_inf error is 2.96e-03\n",
            "\n",
            "=== Builtin least-squares solver, gelsd driver ===\n",
            "CPU times: user 2.05 s, sys: 9.05 ms, total: 2.06 s\n",
            "Wall time: 1.06 s\n",
            "and l_inf error is 4.05e-12\n",
            "\n",
            "=== Builtin least-squares solver, gelsy driver ===\n",
            "CPU times: user 2.92 s, sys: 13 ms, total: 2.93 s\n",
            "Wall time: 1.55 s\n",
            "and l_inf error is 1.10e-12\n",
            "\n",
            "=== Builtin least-squares solver, gelss driver ===\n",
            "CPU times: user 4.29 s, sys: 19.2 ms, total: 4.31 s\n",
            "Wall time: 3.23 s\n",
            "and l_inf error is 3.80e-12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Try it on the GPU\n",
        "\n",
        "Restart the colab runtime and select a GPU runtime (or try a TPU!).  If you register for the Colab \"pro\" account (free for students and educators!) you'll get access to more/faster GPUs.\n",
        "\n",
        "We'll use PyTorch for our GPU computing.  You *can* run this just on the CPU also.  Some code is specific for either CPU or GPU, but most code runs on both.\n",
        "\n",
        "PyTorch has its own `torch.linalg` library, which is similar to (but not the same) as `numpy.linalg` and `scipy.linalg`.  It has GPU implementations, but lacks some of the fancier features that scipy has, and overall has fewer methods.\n",
        "\n",
        "For timing, you can still use `%time` for a single line, or `%%time` to time an entire cell. However, you need to make sure to call `torch.cuda.synchronize()` within the timed code, otherwise it won't time it properly (it will think that the code is done running when it's not really done)."
      ],
      "metadata": {
        "id": "LFxV2P8s5ktY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.linalg as tsla\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print('Device is ', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd2qdr225l65",
        "outputId": "c654f730-bcf7-41e6-e0b0-a469279b99ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is  cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 0: use this for debugging\n",
        "problem_label = 'Problem 0'\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "n       = 20\n",
        "m       = 40\n",
        "A       = torch.randn(m,n,device=device) # by default it's single precision\n",
        "xTrue   = torch.randn(n,1,device=device)\n",
        "b       = A@xTrue\n",
        "print(f'Matrix is size {m} x {n}, device is {device}')\n",
        "print(f'Condition number of A is {tsla.cond(A):.2e}') # 1e3"
      ],
      "metadata": {
        "id": "deyZdZEKtY4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLUTIONS\n",
        "\n",
        "all_methods_torch = []\n",
        "\n",
        "def normal_equations_inverse(A,b):\n",
        "    x = tsla.inv( A.T @ A ) @ (A.T@b )\n",
        "    if b.is_cuda: torch.cuda.synchronize()\n",
        "    return x\n",
        "normal_equations_inverse.label = 'Solve normal equations using the inverse'\n",
        "all_methods_torch.append( normal_equations_inverse )\n",
        "\n",
        "def normal_equations_solve1(A,b):\n",
        "    x = tsla.solve( A.T@A, A.T@b)\n",
        "    if b.is_cuda: torch.cuda.synchronize()\n",
        "    return x\n",
        "normal_equations_solve1.label='Solve normal equations using LU'\n",
        "all_methods_torch.append( normal_equations_solve1 )\n",
        "\n",
        "def normal_equations_solve2(A,b):\n",
        "    if b.is_cuda:\n",
        "        LU, pivots = torch.linalg.lu_factor(A.T@A, pivot=False)\n",
        "        x = torch.linalg.lu_solve( LU, pivots, A.T@b)\n",
        "        torch.cuda.synchronize()\n",
        "        return x\n",
        "    else:\n",
        "        print('lu_factor without pivoting not implemented on CPU, returning zero soln')\n",
        "        return torch.zeros(A.shape[1],1)\n",
        "normal_equations_solve2.label='Solve normal equations using LU, no pivoting'\n",
        "all_methods_torch.append( normal_equations_solve2 )\n",
        "\n",
        "def normal_equations_solve3(A,b):\n",
        "    L = tsla.cholesky(A.T@A) # L@L.T\n",
        "    y = tsla.solve_triangular(L, A.T@b , upper=False)\n",
        "    x = tsla.solve_triangular(L.T, y, upper=True )\n",
        "    if b.is_cuda: torch.cuda.synchronize()\n",
        "    return x\n",
        "normal_equations_solve3.label='Solve normal equations using Cholesky'\n",
        "all_methods_torch.append( normal_equations_solve3 )\n",
        "\n",
        "\n",
        "def QR_nopivoting(A,b):\n",
        "    Q,R = tsla.qr( A, mode=\"reduced\")\n",
        "    Qtb = Q.T@b\n",
        "    x = tsla.solve_triangular(R, Qtb, upper=True )\n",
        "    if b.is_cuda: torch.cuda.synchronize()\n",
        "    return x\n",
        "QR_nopivoting.label='Solve via thin QR, no pivoting'\n",
        "all_methods_torch.append( QR_nopivoting )\n",
        "\n",
        "def builtin_leastsquares(A,b):\n",
        "    x =  tsla.lstsq(A,b,driver='gels')[0]\n",
        "    if b.is_cuda: torch.cuda.synchronize()\n",
        "    return x\n",
        "builtin_leastsquares.label='Builtin least-squares solver'\n",
        "all_methods_torch.append( builtin_leastsquares )"
      ],
      "metadata": {
        "id": "9CZ6Y3Xt512w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem_label = 'Problem 1'\n",
        "torch.manual_seed(1234)\n",
        "n       = int(5e3)\n",
        "m       = n+10\n",
        "A       = torch.randn(m,n,device=device) # by default it's single precision\n",
        "xTrue   = torch.randn(n,1,device=device)\n",
        "b       = A@xTrue\n",
        "print(f'Matrix is size {m} x {n}, device is {device}')\n",
        "print(f'Condition number of A is {tsla.cond(A):.2e}') # 1e3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOWXr7vfIwbb",
        "outputId": "4c86f390-447f-46f9-81ce-279335ec44dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix is size 5010 x 5000, device is cpu\n",
            "Condition number of A is 1.73e+03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "problem_label = 'Problem 2'\n",
        "torch.manual_seed(0)\n",
        "\n",
        "n   = int(2e3)\n",
        "n2  = int(n/2)\n",
        "m   = int(1e4)\n",
        "A       = torch.randn(m,n2,device=device) # by default it's single precision\n",
        "A       = torch.hstack(  (A,A+1e-2*torch.randn(m,n2,device=device) ) )\n",
        "xTrue   = torch.randn(n,1,device=device)\n",
        "b       = A@xTrue\n",
        "print(f'Matrix is size {m} x {n}')\n",
        "print(f'Condition number of A is {tsla.cond(A):.2e}') # 4e2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1oLCITBLtwg",
        "outputId": "df620e9f-2b1b-476a-fb72-fe33b7aec159"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix is size 10000 x 2000\n",
            "Condition number of A is 4.14e+02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(problem_label)\n",
        "print(f'device is {device}')\n",
        "for method in all_methods_torch:\n",
        "    print('=== ' + method.label + ' ===')\n",
        "    %time x = method(A,b)\n",
        "    err   = torch.norm(x-xTrue,float('inf'))\n",
        "    # err   = torch.norm(x-xTrue)\n",
        "    print(f'and l_inf error is {err:.2e}')\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDhZ_IEHJIXx",
        "outputId": "ab5d0d58-1d14-44f1-dd4c-bedba0db3934"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 1\n",
            "device is cpu\n",
            "=== Solve normal equations using the inverse ===\n",
            "CPU times: user 8.94 s, sys: 180 ms, total: 9.11 s\n",
            "Wall time: 2.31 s\n",
            "and l_inf error is 1.87e-01\n",
            "\n",
            "=== Solve normal equations using LU ===\n",
            "CPU times: user 5.55 s, sys: 228 ms, total: 5.77 s\n",
            "Wall time: 1.44 s\n",
            "and l_inf error is 1.02e-01\n",
            "\n",
            "=== Solve normal equations using LU, no pivoting ===\n",
            "lu_factor without pivoting not implemented on CPU, returning zero soln\n",
            "CPU times: user 96 µs, sys: 0 ns, total: 96 µs\n",
            "Wall time: 99.2 µs\n",
            "and l_inf error is 3.75e+00\n",
            "\n",
            "=== Solve normal equations using Cholesky ===\n",
            "CPU times: user 5.78 s, sys: 123 ms, total: 5.9 s\n",
            "Wall time: 1.48 s\n",
            "and l_inf error is 1.55e-01\n",
            "\n",
            "=== Solve via thin QR, no pivoting ===\n",
            "CPU times: user 8.84 s, sys: 186 ms, total: 9.02 s\n",
            "Wall time: 2.32 s\n",
            "and l_inf error is 6.70e-05\n",
            "\n",
            "=== Builtin least-squares solver ===\n",
            "CPU times: user 3.84 s, sys: 57 ms, total: 3.9 s\n",
            "Wall time: 976 ms\n",
            "and l_inf error is 1.03e-04\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU output was:\n",
        "#### Problem 1\n",
        "```\n",
        "Problem 1\n",
        "device is cpu\n",
        "=== Solve normal equations using the inverse ===\n",
        "CPU times: user 8.94 s, sys: 180 ms, total: 9.11 s\n",
        "Wall time: 2.31 s\n",
        "and l_inf error is 1.87e-01\n",
        "\n",
        "=== Solve normal equations using LU ===\n",
        "CPU times: user 5.55 s, sys: 228 ms, total: 5.77 s\n",
        "Wall time: 1.44 s\n",
        "and l_inf error is 1.02e-01\n",
        "\n",
        "=== Solve normal equations using Cholesky ===\n",
        "CPU times: user 5.78 s, sys: 123 ms, total: 5.9 s\n",
        "Wall time: 1.48 s\n",
        "and l_inf error is 1.55e-01\n",
        "\n",
        "=== Solve via thin QR, no pivoting ===\n",
        "CPU times: user 8.84 s, sys: 186 ms, total: 9.02 s\n",
        "Wall time: 2.32 s\n",
        "and l_inf error is 6.70e-05\n",
        "\n",
        "=== Builtin least-squares solver ===\n",
        "CPU times: user 3.84 s, sys: 57 ms, total: 3.9 s\n",
        "Wall time: 976 ms\n",
        "and l_inf error is 1.03e-04\n",
        "```\n",
        "#### Problem 2\n",
        "```\n",
        "Problem 2\n",
        "device is cpu\n",
        "=== Solve normal equations using the inverse ===\n",
        "CPU times: user 1.69 s, sys: 13.9 ms, total: 1.7 s\n",
        "Wall time: 429 ms\n",
        "and l_inf error is 1.08e-01\n",
        "\n",
        "=== Solve normal equations using LU ===\n",
        "CPU times: user 1.51 s, sys: 26.1 ms, total: 1.54 s\n",
        "Wall time: 398 ms\n",
        "and l_inf error is 7.40e-02\n",
        "\n",
        "=== Solve normal equations using Cholesky ===\n",
        "CPU times: user 1.31 s, sys: 25.9 ms, total: 1.34 s\n",
        "Wall time: 334 ms\n",
        "and l_inf error is 1.07e-01\n",
        "\n",
        "=== Solve via thin QR, no pivoting ===\n",
        "CPU times: user 3.46 s, sys: 103 ms, total: 3.56 s\n",
        "Wall time: 892 ms\n",
        "and l_inf error is 1.76e-04\n",
        "\n",
        "=== Builtin least-squares solver ===\n",
        "CPU times: user 1.84 s, sys: 69 ms, total: 1.9 s\n",
        "Wall time: 476 ms\n",
        "and l_inf error is 1.10e-04\n",
        "```\n",
        "\n",
        "## GPU (L40) output\n",
        "#### Problem 1\n",
        "```\n",
        "Problem 1\n",
        "device is cuda\n",
        "=== Solve normal equations using the inverse ===\n",
        "CPU times: user 79.8 ms, sys: 0 ns, total: 79.8 ms\n",
        "Wall time: 79.5 ms\n",
        "and l_inf error is 8.65e-02\n",
        "\n",
        "=== Solve normal equations using LU ===\n",
        "CPU times: user 56 ms, sys: 0 ns, total: 56 ms\n",
        "Wall time: 56 ms\n",
        "and l_inf error is 3.42e-02\n",
        "\n",
        "=== Solve normal equations using LU, no pivoting ===\n",
        "CPU times: user 39.8 ms, sys: 0 ns, total: 39.8 ms\n",
        "Wall time: 39.8 ms\n",
        "and l_inf error is 4.36e-02\n",
        "\n",
        "=== Solve normal equations using Cholesky ===\n",
        "CPU times: user 34.9 ms, sys: 859 µs, total: 35.8 ms\n",
        "Wall time: 35.2 ms\n",
        "and l_inf error is 4.08e-02\n",
        "\n",
        "=== Solve via thin QR, no pivoting ===\n",
        "CPU times: user 66 ms, sys: 27 ms, total: 93 ms\n",
        "Wall time: 93 ms\n",
        "and l_inf error is 9.88e-05\n",
        "\n",
        "=== Builtin least-squares solver ===\n",
        "CPU times: user 64.3 ms, sys: 13 ms, total: 77.3 ms\n",
        "Wall time: 77.3 ms\n",
        "and l_inf error is 9.10e-05\n",
        "```\n",
        "#### Problem 2\n",
        "```\n",
        "Problem 2\n",
        "device is cuda\n",
        "=== Solve normal equations using the inverse ===\n",
        "CPU times: user 14.3 ms, sys: 39 µs, total: 14.4 ms\n",
        "Wall time: 14.1 ms\n",
        "and l_inf error is 1.52e-01\n",
        "\n",
        "=== Solve normal equations using LU ===\n",
        "CPU times: user 10.5 ms, sys: 0 ns, total: 10.5 ms\n",
        "Wall time: 10.5 ms\n",
        "and l_inf error is 1.17e-01\n",
        "\n",
        "=== Solve normal equations using LU, no pivoting ===\n",
        "CPU times: user 6.84 ms, sys: 0 ns, total: 6.84 ms\n",
        "Wall time: 6.84 ms\n",
        "and l_inf error is 3.07e-01\n",
        "\n",
        "=== Solve normal equations using Cholesky ===\n",
        "CPU times: user 8.02 ms, sys: 0 ns, total: 8.02 ms\n",
        "Wall time: 8.02 ms\n",
        "and l_inf error is 3.21e-01\n",
        "\n",
        "=== Solve via thin QR, no pivoting ===\n",
        "CPU times: user 50.4 ms, sys: 0 ns, total: 50.4 ms\n",
        "Wall time: 50.4 ms\n",
        "and l_inf error is 1.44e-04\n",
        "\n",
        "=== Builtin least-squares solver ===\n",
        "CPU times: user 44.7 ms, sys: 0 ns, total: 44.7 ms\n",
        "Wall time: 44.7 ms\n",
        "and l_inf error is 8.51e-05\n",
        "```"
      ],
      "metadata": {
        "id": "UHdPkZVMNYEk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUV5BK00JiUH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}